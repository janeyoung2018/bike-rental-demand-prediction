{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-Xf3TUbRzZC"
   },
   "source": [
    "# Exploratory Data Analysis â€” Bike Sharing Dataset\n",
    "\n",
    "**Goal**: Understand patterns in hourly bike rentals (`cnt`) to support building a daily prediction service model for planning processes and bicycle logistics.\n",
    "\n",
    "Dataset source: [UCI Bike Sharing Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "- [Load & Preview Data](#1-load-and-preview-data)\n",
    "- [Basic Cleaning Preprocessing](#2-basic-cleaning--preprocessing)\n",
    "- [Statistics Analysis](#3-statistics-analysis)\n",
    "- [Time Series Analysis](#4-time-series-analysis)\n",
    "- [Model Selection](#5-model-selection)\n",
    "- [Model Evaluation](#6-model-evaluation)\n",
    "- [Feature Importance](#7-feature-importance)\n",
    "- [Bonus Questions](#8-bonus-questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EFXkbXzxWzB5"
   },
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import plotly.graph_objects as go\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "from utils_nb import *\n",
    "from matplotlib import pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_show_fig(fig, title):\n",
    "    \"\"\"Helper function to create and show figures with consistent layout.\"\"\"\n",
    "    fig.update_layout(width=600, height=350, title=title)  # Set figure size here\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3h-IGsR9UVow"
   },
   "source": [
    "## 1. Load and Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "id": "sS4VjfTHRwGi",
    "outputId": "544525db-32c6-426f-edc6-9a2b3d21a81f"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/hour.csv\")  # 'hour.csv' is the hourly dataset\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data status, columns, dtypes, non-null count\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5Lb-8UEWiUN"
   },
   "source": [
    "## 2. Basic Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwiqT9VvQnIH"
   },
   "source": [
    "### 2.1. Convert 'dteday' to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E1szcaK_QeHw"
   },
   "outputs": [],
   "source": [
    "# Convert date column\n",
    "df['dteday'] = pd.to_datetime(df['dteday'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqEe6NM_QmiH"
   },
   "source": [
    "### 2.2. Check  missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Sb2ZgxJRWPp",
    "outputId": "adc2dfb7-15e9-4cc5-dc10-165066b45495"
   },
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing_values = df.isnull().values.any()\n",
    "print(f\"Missing values in dataset: {missing_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-RhQEPEQsNB"
   },
   "source": [
    "### 2.3 Check Dataset Coverage & Completeness\n",
    "- A few days contain fewer than 24 records, indicating data incompleteness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSh2erveR3Sz",
    "outputId": "ed82c991-7e90-49ec-9a16-8847e47b2def"
   },
   "outputs": [],
   "source": [
    "# Check the overall date coverage\n",
    "print(\"Date Range:\")\n",
    "print(f\"Start: {df['dteday'].min().date()}  |  End: {df['dteday'].max().date()}\")\n",
    "\n",
    "# Count number of unique days\n",
    "num_days = df['dteday'].nunique()\n",
    "print(f\"Total unique days: {num_days}\")\n",
    "\n",
    "# Show available years\n",
    "years = df['dteday'].dt.year.unique()\n",
    "print(f\"Years in dataset: {sorted(years)}\")\n",
    "\n",
    "# Check how many records per day (should be 24 ideally)\n",
    "daily_record_unique_counts = df.groupby('dteday')['instant'].count().value_counts()\n",
    "print(\"\\nUnique records per day:\")\n",
    "print(daily_record_unique_counts)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check duplicates dates and hours\n",
    "dupli_num = df.duplicated(subset=['dteday', 'hr'], keep='last').shape[0] - df.shape[0]\n",
    "print(f\"Duplicated records number: {dupli_num}\")\n",
    "\n",
    "# Check cnt=0 records\n",
    "zero_cnt_num = df[df['cnt'] == 0].shape[0]\n",
    "print(f\"Zero cnt records number: {zero_cnt_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "nlyP9z6_XGYb",
    "outputId": "6aa26e89-d43e-4046-ece3-61e5df8906d7"
   },
   "outputs": [],
   "source": [
    "# Clean and fill missing hours\n",
    "df_cleaned = fill_missing_hours(df)\n",
    "\n",
    "# Confirm daily completeness\n",
    "assert all(df_cleaned.groupby('dteday')['cnt'].count() == 24)\n",
    "\n",
    "# Confirm no nulls left\n",
    "print(df_cleaned.isnull().any().any())\n",
    "\n",
    "# Preview cleaned data\n",
    "df_cleaned.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistics Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 466
    },
    "id": "F4-15pCZZhin",
    "outputId": "82b85ba9-d795-45e6-eac2-b00adec2d9ff"
   },
   "outputs": [],
   "source": [
    "# Target Variable (cnt)\n",
    "fig_cnt_density = px.histogram(df_cleaned, x='cnt', marginal='rug', title = f'Distribution & Density plot of cnt')\n",
    "fig_cnt_box = px.box(df_cleaned, y=\"cnt\", title=\"Box plot of cnt\")\n",
    "create_and_show_fig(fig_cnt_density, 'Distribution & Density plot of cnt')\n",
    "create_and_show_fig(fig_cnt_box, title=\"Box plot of cnt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tB59ntMDb-Lq"
   },
   "outputs": [],
   "source": [
    "# Numerical Features\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "for feature in numerical_features:\n",
    "    fig_box = px.box(df_cleaned, y=feature, title=f\"Box plot of {feature}\")\n",
    "    fig_density = px.histogram(df_cleaned, x=feature, marginal='rug', title = f'Distribution & Density plot of {feature}')\n",
    "    create_and_show_fig(fig_density, f\"Distribution & Density plot of {feature}\")\n",
    "    create_and_show_fig(fig_box, f\"Box plot of {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical Features\n",
    "# The distribution of ['season', 'yr', 'mnth', 'hr', 'weekday'] is straightforward, ignore here\n",
    "categorical_features =  ['workingday', 'weathersit']\n",
    "for feature in categorical_features:\n",
    "    fig_bar = px.bar(df_cleaned[feature].value_counts().sort_index(), title=f\"Count of {feature}\")\n",
    "    create_and_show_fig(fig_bar, f\"Count of {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Bivariate Analysis\n",
    "# Target vs. Numerical\n",
    "numerical_features = ['temp', 'atemp', 'hum', 'windspeed']\n",
    "for feature in numerical_features:\n",
    "    fig_scatter = px.scatter(df_cleaned, x=feature, y=\"cnt\", title=f\"cnt vs. {feature}\")\n",
    "    create_and_show_fig(fig_scatter, f\"cnt vs. {feature}\")\n",
    "\n",
    "fig_line_hr = px.line(df_cleaned.groupby('hr')['cnt'].mean().reset_index(), x='hr', y='cnt', title = 'Mean cnt by hour')\n",
    "create_and_show_fig(fig_line_hr, 'Mean cnt by hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target vs. Categorical\n",
    "categorical_features = ['season', 'yr', 'mnth', 'weekday', 'workingday', 'weathersit']\n",
    "for feature in categorical_features:\n",
    "    fig_box_cat = px.box(df_cleaned, x=feature, y=\"cnt\", title=f\"cnt vs. {feature}\")\n",
    "    create_and_show_fig(fig_box_cat, f\"cnt vs. {feature}\")\n",
    "    fig_bar_mean = px.bar(df_cleaned.groupby(feature)['cnt'].mean(), title = f'Mean cnt by {feature}')\n",
    "    create_and_show_fig(fig_bar_mean, f'Mean cnt by {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "correlation_matrix = df_cleaned[numerical_features + ['cnt']].corr()\n",
    "fig_heatmap = px.imshow(correlation_matrix, title=\"Correlation Matrix\")\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Count Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt over time\n",
    "fig_time_series = px.line(df_cleaned, x='dteday', y='cnt', title='cnt over time')\n",
    "create_and_show_fig(fig_time_series, \"cnt over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Daily Count Seasonal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned['dteday'] = pd.to_datetime(df_cleaned['dteday'])\n",
    "# Use resample to set the frequency and aggregate to daily sums\n",
    "daily_cnt = df_cleaned.set_index('dteday')['cnt'].resample('D').sum() #set index, and resample.\n",
    "\n",
    "# Seasonal Decomposition\n",
    "decomposition = seasonal_decompose(daily_cnt, model=\"additive\", period=30)\n",
    "\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plotting the Decomposition\n",
    "fig_original = px.line(x=daily_cnt.index, y=daily_cnt, title=\"Original Time Series\")\n",
    "create_and_show_fig(fig_original, \"Original Time Series\")\n",
    "\n",
    "fig_trend = px.line(x=trend.index, y=trend, title=\"Trend Component\")\n",
    "create_and_show_fig(fig_trend, \"Trend Component\")\n",
    "\n",
    "fig_seasonal = px.line(x=seasonal.index, y=seasonal, title=\"Seasonal Component\")\n",
    "create_and_show_fig(fig_seasonal, \"Seasonal Component\")\n",
    "\n",
    "fig_residual = px.line(x=residual.index, y=residual, title=\"Residual Component\")\n",
    "create_and_show_fig(fig_residual, \"Residual Component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily cnt ACF and PACF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ACF plot\n",
    "plt.subplot(211)\n",
    "plot_acf(daily_cnt, lags=730, ax=plt.gca()) #lags set to 365, to show a years worth of data.\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# PACF plot\n",
    "plt.subplot(212)\n",
    "plot_pacf(daily_cnt, lags=365,ax=plt.gca()) #lags set to 30, to show a month worth of data.\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Hourly Count ACF and PACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_cnt = df_cleaned.groupby(\"datetime\").cnt.sum()\n",
    "\n",
    "# Apply Differencing\n",
    "hourly_diff = hourly_cnt.diff(24).dropna()\n",
    "\n",
    "check_stationarity(hourly_cnt)\n",
    "check_stationarity(hourly_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ACF and PACF\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# ACF plot\n",
    "plt.subplot(211)\n",
    "plot_acf(hourly_cnt, lags=24, ax=plt.gca()) #lags set to 365, to show a years worth of data.\n",
    "plt.title('Autocorrelation Function (ACF)')\n",
    "\n",
    "# PACF plot\n",
    "plt.subplot(212)\n",
    "plot_pacf(hourly_cnt, lags=24,ax=plt.gca()) #lags set to 30, to show a month worth of data.\n",
    "plt.title('Partial Autocorrelation Function (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot hourly_cnt and hourly_diff ACF and PACF using Plotly\n",
    "\n",
    "plot_acf_pacf_plotly(hourly_cnt, lags=72)\n",
    "plot_acf_pacf_plotly(hourly_diff, lags=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Selection\n",
    "- Rolling average model (Look back 1 to 7 weeks (same day and hour))\n",
    "- Arima model, applied 24 hours seasonal differencing and (p, d, q)=(2, 0, 3)\n",
    "- Xgboost model with lag_24, lag_48, lag_168.\n",
    "- Test period (\"2012-08-01\", \"2012-08-07\") and (\"2012-12-25\", \"2012-12-31\")(only for save time purpose, not a fair comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weeks = [\n",
    "    (\"2012-08-01\", \"2012-08-07\"),\n",
    "    (\"2012-12-25\", \"2012-12-31\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all models\n",
    "baseline_df = evaluate_baseline(hourly_cnt, test_weeks)\n",
    "arima_df = evaluate_arima(hourly_cnt, test_weeks)\n",
    "xgb_df = evaluate_xgboost_weekly(df_cleaned, test_weeks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([arima_df, xgb_df,baseline_df])\n",
    "results = label_test_period(results, test_weeks)\n",
    "results['date'] = pd.to_datetime(results['date'])\n",
    "results = results.sort_values(by=[\"model\", \"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting MAE for each model over the test weeks\n",
    "\n",
    "# Get the unique test periods\n",
    "test_periods = results['test_period'].unique()\n",
    "\n",
    "model_colors = {\n",
    "    'ARIMA': '#636EFA',\n",
    "    'Baseline': '#EF553B',\n",
    "    'XGBoost': '#00CC96'\n",
    "}\n",
    "\n",
    "# Create subplots (1 row per test week)\n",
    "fig = make_subplots(\n",
    "    rows=len(test_periods),\n",
    "    cols=1,\n",
    "    shared_xaxes=False,\n",
    "    subplot_titles=test_periods\n",
    ")\n",
    "\n",
    "for i, period in enumerate(test_periods):\n",
    "    df_sub = results[results['test_period'] == period]\n",
    "    models = df_sub['model'].unique()\n",
    "\n",
    "    for model in models:\n",
    "        df_model = df_sub[df_sub['model'] == model]\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=df_model['date'],\n",
    "                y=df_model['mae'],\n",
    "                mode='lines+markers',\n",
    "                name=model,\n",
    "                legendgroup=model,\n",
    "                showlegend=(i == 0),  # only show legend once\n",
    "                line=dict(color=model_colors.get(model, 'gray'))  # << add this\n",
    "            ),\n",
    "            row=i+1,\n",
    "            col=1\n",
    "        )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=500 * len(test_periods),\n",
    "    title_text=\"Model MAE Over Selected Test Weeks (Separate X-Axis)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"MAE\",\n",
    "    legend_title=\"Model\"\n",
    ")\n",
    "\n",
    "fig.update_xaxes(tickformat=\"%b %d\", tickangle=45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare mean MAE per model\n",
    "print(results.groupby('model')['mae'].mean().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation\n",
    "<pre>\n",
    "XGBoost Model Design:\n",
    "24 individual XGBoost models were trained, one for each hour of the day, to capture hourly specific patterns.\n",
    "Features included the provided dataset features, as well as engineered features: \"lag_1\", \"rolling_24\", \"rolling_168\", \"day_mean\", \"day_max\", \"day_min\", \"day_std\", and \"day_sum\", to incorporate temporal dependencies and daily aggregations.\n",
    "Data Split\n",
    "Training Set: Before '2012-07-01'\n",
    "Validation Set: '2012-07-01' to '2012-10-01'\n",
    "Test Set: After â€˜2012-10-01'\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the prediction data\n",
    "# the data is included in the repo, but you can also generate it via python src/predict.py\n",
    "# or download it from the repo. If is the former, please make sure the date in the filename is correct.\n",
    "df_eval = pd.read_csv(\"../data/prediction/test/pred_20250404.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test date range\n",
    "df_eval['dteday'] = pd.to_datetime(df_eval['dteday'])\n",
    "print(df_eval['dteday'].dt.date.min(), df_eval['dteday'].dt.date.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily average mae\n",
    "daily_maes = {}\n",
    "for day in df_eval['dteday'].dt.date.unique():\n",
    "    day_df = df_eval[df_eval['dteday'].dt.date == day]\n",
    "    cnt_hourly = []\n",
    "    cnt_hourly_pred = []\n",
    "    for i in range(1, 25):\n",
    "        cnt_hourly.append(day_df[f't+{i}'].values[0])\n",
    "        cnt_hourly_pred.append(day_df[f't+{i}_pred'].values[0])\n",
    "    cnt_hourly = np.array(cnt_hourly)\n",
    "    cnt_hourly_pred = np.array(cnt_hourly_pred)\n",
    "    # Calculate MAE\n",
    "    mae = mean_absolute_error(cnt_hourly, cnt_hourly_pred)\n",
    "    daily_maes[day] = mae\n",
    "# avergage mae\n",
    "avg_mae = np.mean(list(daily_maes.values()))\n",
    "print(f\"Average MAE: {avg_mae:.2f}\")\n",
    "# plot daily mae\n",
    "fig_daily_mae = px.line(\n",
    "    x=list(daily_maes.keys()),\n",
    "    y=list(daily_maes.values()),\n",
    "    title=\"Daily MAE\",\n",
    "    labels={'x': 'Date', 'y': 'MAE'},\n",
    "    markers=True\n",
    ")\n",
    "# Add a horizontal line for the average MAE\n",
    "fig_daily_mae.add_hline(y=avg_mae, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Average MAE\", annotation_position=\"top right\")\n",
    "# Update x-axis to show dates properly\n",
    "fig_daily_mae.update_traces(marker=dict(size=5))\n",
    "fig_daily_mae.update_xaxes(\n",
    "    tickformat=\"%Y-%m-%d\",\n",
    "    tickangle=-45,\n",
    "    # dtick=\"W2\",  # Show one tick per month\n",
    "    # tickmode='linear'\n",
    ")\n",
    "# Update layout\n",
    "fig_daily_mae.update_layout(\n",
    "    title=\"Daily Mean Absolute Error (MAE)\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"MAE\",\n",
    "    legend_title_text='Model',\n",
    "    height=400\n",
    ")\n",
    "fig_daily_mae.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily cnt compared to prediction\n",
    "daily_cnts = {}\n",
    "for day in df_eval['dteday'].dt.date.unique():\n",
    "    day_df = df_eval[df_eval['dteday'].dt.date == day]\n",
    "    daily_cnt = 0\n",
    "    daily_cnt_pred = 0\n",
    "    for i in range(1, 25):\n",
    "        daily_cnt += day_df[f't+{i}'].values[0]\n",
    "        daily_cnt_pred += day_df[f't+{i}_pred'].values[0]\n",
    "    daily_cnts[day] = (daily_cnt, daily_cnt_pred)\n",
    "# Convert to DataFrame\n",
    "df_daily_cnts = pd.DataFrame(list(daily_cnts.items()), columns=['date', 'cnt'])\n",
    "df_daily_cnts[['cnt', 'cnt_pred']] = pd.DataFrame(df_daily_cnts['cnt'].tolist(), index=df_daily_cnts.index)\n",
    "# Plotting the daily cnt compared to prediction\n",
    "fig_daily_cnt = px.line(\n",
    "    df_daily_cnts,\n",
    "    x='date',\n",
    "    y=['cnt', 'cnt_pred'],\n",
    "    title=\"Daily cnt vs. Daily cnt Prediction\",\n",
    "    labels={'x': 'Date', 'y': 'cnt'},\n",
    "    markers=True\n",
    ")\n",
    "# Update x-axis to show dates properly\n",
    "fig_daily_cnt.update_traces(marker=dict(size=5))\n",
    "# Update layout\n",
    "fig_daily_cnt.update_layout(\n",
    "    title=\"Daily cnt vs. Daily cnt Prediction\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"cnt\",\n",
    "    legend_title_text='Model',\n",
    "    height=400\n",
    ")\n",
    "fig_daily_cnt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 8am(t+9), 12pm(t+13), 5pm(t+18) saved model\n",
    "# Please note that the model is not saved in the repo, but you can generate it via python src/train.py\n",
    "# when load the generated model, please make sure the date in the filename is correct.\n",
    "model = xgb.XGBRegressor()\n",
    "model.load_model(\"../models/models_20250402/model_t+18.json\")\n",
    "\n",
    "# Get the booster from the sklearn wrapper\n",
    "booster = model.get_booster()\n",
    "\n",
    "# set real feature names if available\n",
    "# booster.feature_names = ['age', 'income', 'gender', 'education']\n",
    "\n",
    "# Get feature importance scores (by gain)\n",
    "importance = booster.get_score(importance_type='gain')\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_importance = pd.DataFrame({\n",
    "    'Feature': list(importance.keys()),\n",
    "    'Importance': list(importance.values())\n",
    "}).sort_values(by=\"Importance\", ascending=False).head(10)\n",
    "\n",
    "# Plot with Plotly\n",
    "fig = px.bar(\n",
    "    df_importance,\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    orientation='h',\n",
    "    title=\"XGBoost Feature Importance 5pm (by Gain)\",\n",
    "    height=400\n",
    ")\n",
    "fig.update_layout(yaxis=dict(autorange=\"reversed\"))  # Highest on top\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Bonus Questions\n",
    "\n",
    "- **Q1:** Which intervals would you suggest to refresh the forecasts during operations? Why?  \n",
    "  To optimize bicycle rental operations, I suggest a tiered forecast refresh strategy:\n",
    "\n",
    "  - **Daily Hourly Updates:** Crucial for immediate logistics, capturing daily weather fluctuations (most reliable within 1â€“3 day weather forecast range).\n",
    "  - **Weekly Aggregated Refreshes:** For mid-term planning (staffing, maintenance), balancing trends with decreasing weather forecast accuracy.\n",
    "  - **Event-Driven Refreshes:** Triggered by significant weather changes, events, or data anomalies, ensuring rapid operational adjustments.\n",
    "  - **Monthly/Quarterly Scenario Outlooks:** For long-term strategic planning, providing general trends while acknowledging the inherent uncertainty of long-range weather forecasts.\n",
    "\n",
    "  This strategy prioritizes short-term accuracy for operational efficiency and uses long-term outlooks for strategic guidance, with clear consideration for weather forecast reliability.\n",
    "\n",
    "- **Q2:** How far out into the future is your model good for planning? Why?  \n",
    "  With a simple ARIMA prediction, the difference between the prediction termination day and the start day is approximately **3 months (86 days)**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split to Train and Test\n",
    "train_size = int(len(hourly_cnt) * 0.8)\n",
    "train, test = hourly_cnt[:train_size], hourly_cnt[train_size:]\n",
    "\n",
    "# 3. Apply Seasonal Differencing\n",
    "seasonal_lag = 24\n",
    "train_diff = train.diff(seasonal_lag).dropna()\n",
    "check_stationarity(train_diff)\n",
    "\n",
    "# 4. Fit SARIMA Model (Example)\n",
    "model = ARIMA(train_diff, order=(2, 0, 3))\n",
    "results = model.fit()\n",
    "\n",
    "# 5. Forecast Train and Test\n",
    "train_forecast_diff = results.fittedvalues\n",
    "test_forecast_diff = results.forecast(steps=len(test))\n",
    "\n",
    "# 6. Inverse Transformation for Train\n",
    "train_forecast_original = pd.Series(index=train_diff.index)\n",
    "\n",
    "# Initialize the first 24 values\n",
    "for i in range(seasonal_lag):\n",
    "    train_forecast_original[train_diff.index[i]] = train[train_diff.index[i]]\n",
    "\n",
    "# Inverse transform the rest of the values\n",
    "for i in range(seasonal_lag, len(train_diff)):\n",
    "    train_forecast_original[train_diff.index[i]] = train_forecast_original[train_diff.index[i - seasonal_lag]] + train_forecast_diff[i-seasonal_lag]\n",
    "\n",
    "# 7. Inverse Transformation for Test\n",
    "test_forecast_original = pd.Series(index=test.index)\n",
    "\n",
    "# Initialize the first 24 values using the last 24 values from train\n",
    "for i in range(seasonal_lag):\n",
    "    test_forecast_original[test.index[i]] = train[-seasonal_lag + i] + test_forecast_diff[i]\n",
    "\n",
    "# Inverse transform the rest of the values\n",
    "for i in range(seasonal_lag, len(test_forecast_diff)):\n",
    "    test_forecast_original[test.index[i]] = test_forecast_original[test.index[i - seasonal_lag]] + test_forecast_diff[i]\n",
    "\n",
    "# 8. Plot Train and Test Predictions\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(train, label='Train Data')\n",
    "plt.plot(train_forecast_original, label='Train Forecast', color='green')\n",
    "plt.show()\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.plot(test, label='Test Data', color='blue')\n",
    "plt.plot(test_forecast_original, label='Test Forecast', color='red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "mae_train = mean_absolute_error(train[24:], train_forecast_original)\n",
    "mae_test = mean_absolute_error(test, test_forecast_original)\n",
    "print(f\"Train mae: {mae_train}\")\n",
    "print(f\"Test mae: {mae_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot test and forecast devation\n",
    "test_daily = test.resample('D').sum()\n",
    "test_forecast_original_daily = test_forecast_original.resample('D').sum()\n",
    "\n",
    "# Calculate absolute deviation\n",
    "deviation = (test_daily - test_forecast_original_daily).abs()\n",
    "\n",
    "# Create the Plotly figure\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x = test_daily.index,\n",
    "    y=deviation,\n",
    "    mode='lines',\n",
    "    name='Test Data actual vs pred deviation',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Test Data actual vs pred deviation',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Absolute Deviation',\n",
    "    width=600,\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date1 = datetime(2012, 11, 1)\n",
    "date2 = datetime(2012, 8, 7)\n",
    "\n",
    "# Calculate the difference\n",
    "day_difference = (date1 - date2).days\n",
    "\n",
    "print(f\"Day difference: {day_difference} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
